"""
Ultimate API server fuer DLH Chatbot - BEST OF BOTH WORLDS VERSION
Combines:
- System Prompt for guaranteed links (new)
- Metadata subject search (new)
- Sonnet 4.5 (new)
- Overview URL prioritization (old, essential!)
- Event chronological sorting (old, essential!)
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Tuple
import json
import os
import re
import uvicorn
import logging
import requests

# === Added: robust German date parsing and workshop scraping ===
from datetime import datetime, timedelta, timezone
import urllib.parse
import re as _re

DE_MONTHS = {
    'januar': 1, 'jan': 1,
    'februar': 2, 'feb': 2,
    'maerz': 3, 'märz': 3, 'mrz': 3, 'maer': 3, 'mar': 3,
    'april': 4, 'apr': 4,
    'mai': 5,
    'juni': 6, 'jun': 6,
    'juli': 7, 'jul': 7,
    'august': 8, 'aug': 8,
    'september': 9, 'sep': 9, 'sept': 9,
    'oktober': 10, 'okt': 10,
    'november': 11, 'nov': 11,
    'dezember': 12, 'dez': 12
}

def _normalize_dash(s: str) -> str:
    return s.replace('\u2013', '-').replace('\u2014', '-').replace('–', '-').replace('—', '-')

def parse_de_date_text(txt: str):
    if not txt:
        return None
    s = _normalize_dash(txt.lower().strip())
    s = s.replace('uhr', '').replace(',', ' ')
    m = _re.search(r'(\d{1,2})\s*(?:\.\s*)?(jan|januar|feb|februar|maerz|märz|mrz|mar|april|apr|mai|jun|juni|jul|juli|aug|august|sep|sept|september|okt|oktober|nov|november|dez|dezember)\s*(\d{4})', s)
    if not m:
        m = _re.search(r'(\d{1,2})\.(\d{1,2})\.(\d{4})', s)
        if m:
            day, month, year = int(m.group(1)), int(m.group(2)), int(m.group(3))
        else:
            return None
    else:
        day = int(m.group(1))
        mon = m.group(2)
        month = DE_MONTHS.get(mon, None)
        if not month:
            return None
        year = int(m.group(3))
    tmatch = _re.search(r'(\d{1,2}):(\d{2})', s)
    hour, minute = (0, 0)
    if tmatch:
        hour, minute = int(tmatch.group(1)), int(tmatch.group(2))
    try:
        return datetime(year, month, day, hour, minute)
    except Exception:
        return None

def dedupe_items(items, key=lambda x: (x.get('title','').lower().strip(), x.get('when',''))):
    seen = set()
    out = []
    for it in items:
        k = key(it)
        if k in seen:
            continue
        seen.add(k)
        out.append(it)
    return out

def get_upcoming_impuls_workshops_live(max_items: int = 8):
    url = "https://dlh.zh.ch/home/impuls-workshops"
    try:
        resp = requests.get(url, timeout=10, headers={"User-Agent": "DLH-Chatbot/1.0"})
        resp.raise_for_status()
    except Exception:
        return []

    from bs4 import BeautifulSoup
    soup = BeautifulSoup(resp.text, "html.parser")
    candidates = soup.select("li, .event, .teaser, .item, article")
    events = []
    now = datetime.now()
    for el in candidates:
        a = el.find("a")
        title = a.get_text(strip=True) if a else el.get_text(" ", strip=True)[:120]
        href = a.get("href") if a and a.has_attr("href") else url
        full_url = urllib.parse.urljoin(url, href)

        dt_el = None
        for css in ["time", ".date", ".datetime", ".termine", ".event-date"]:
            dt_el = el.select_one(css) if hasattr(el, "select_one") else None
            if dt_el:
                break
        when_text = (dt_el.get_text(" ", strip=True) if dt_el else el.get_text(" ", strip=True))
        when_text = _normalize_dash(when_text)
        dt = parse_de_date_text(when_text)
        if not dt or dt < now:
            continue

        desc_el = None
        for css in [".intro", ".desc", "p"]:
            desc_el = el.select_one(css) if hasattr(el, "select_one") else None
            if desc_el:
                break
        snippet = (desc_el.get_text(" ", strip=True) if desc_el else "")

        if _re.search(r"impuls|workshop|reihe|mintwoch|one change", (title + " " + when_text).lower()):
            events.append({
                "title": title,
                "url": full_url,
                "when": dt.isoformat(),
                "when_text": when_text,
                "snippet": snippet
            })
    events = dedupe_items(events)
    events.sort(key=lambda e: e["when"])
    return events[:max_items]


from anthropic import Anthropic
from dotenv import load_dotenv
from datetime import datetime, timedelta
from collections import Counter

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Initialize FastAPI app
app = FastAPI(
    title="DLH Chatbot API (Best of Both Worlds)",
    description="System Prompt + Metadata Search + Overview Priority",
    version="4.0.0"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize Anthropic client
anthropic_client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

# Load and preprocess data
def load_and_preprocess_data():
    """Lade und bereite Daten mit verbesserter Struktur vor"""
    try:
        file_path = 'processed/processed_chunks.json'
        
        print(f"Y Attempting to load data from: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            chunks = json.load(f)
        
        print(f" Successfully loaded {len(chunks)} chunks from {file_path}")
        
        # Erstelle Index fuer schnellere Suche
        keyword_index = {}
        url_index = {}
        subject_index = {}
        
        for i, chunk in enumerate(chunks):
            # URL-basierter Index
            url = chunk['metadata'].get('source', '').lower()
            if url not in url_index:
                url_index[url] = []
            url_index[url].append(i)
            
            # FAcher-Index aus Metadaten
            faecher = chunk['metadata'].get('faecher', [])
            if faecher:
                for fach in faecher:
                    fach_lower = fach.lower()
                    if fach_lower not in subject_index:
                        subject_index[fach_lower] = []
                    subject_index[fach_lower].append(i)
            
            # Keyword-Index
            content = chunk['content'].lower()
            important_terms = [
                'fobizz', 'genki', 'innovationsfonds', 'cop', 'cops',
                'vernetzung', 'workshop', 'weiterbildung', 'kuratiert',
                'impuls', 'termin', 'anmeldung', 'lunch', 'learn',
                'impuls-workshop', 'impulsworkshop', 'veranstaltung', 'event',
                'chemie', 'physik', 'biologie', 'mathematik', 'informatik',
                'deutsch', 'englisch', 'franzAsisch', 'italienisch', 'spanisch',
                'geschichte', 'geografie', 'wirtschaft', 'recht', 'philosophie'
            ]
            
            for term in important_terms:
                if term in content:
                    if term not in keyword_index:
                        keyword_index[term] = []
                    keyword_index[term].append(i)
        
        print(f"Y Indexed {len(keyword_index)} keywords")
        print(f"Ys Indexed {len(subject_index)} subjects in metadata")
        
        return chunks, keyword_index, url_index, subject_index
    except Exception as e:
        print(f"a Error loading data: {e}")
        import traceback
        traceback.print_exc()
        return [], {}, {}, {}

# Global data storage
CHUNKS, KEYWORD_INDEX, URL_INDEX, SUBJECT_INDEX = load_and_preprocess_data()

class QuestionRequest(BaseModel):
    question: str
    language: Optional[str] = "de"
    max_sources: Optional[int] = 5

class Source(BaseModel):
    url: str
    title: str
    snippet: str

class AnswerResponse(BaseModel):
    question: str
    answer: str
    sources: List[Source]

def extract_dates_from_text(text: str) -> List[Tuple[datetime, str]]:
    """Extrahiere Daten aus Text - unterstA14tzt auch abgekA14rzte Monatsnamen"""
    dates_found = []
    
    month_map_full = {
        'januar': 1, 'februar': 2, 'maerz': 3, 'april': 4,
        'mai': 5, 'juni': 6, 'juli': 7, 'august': 8,
        'september': 9, 'oktober': 10, 'november': 11, 'dezember': 12
    }
    
    month_map_abbr = {
        'jan': 1, 'feb': 2, 'mAr': 3, 'maerz': 3, 'mrz': 3, 'apr': 4,
        'mai': 5, 'jun': 6, 'jul': 7, 'aug': 8,
        'sep': 9, 'sept': 9, 'okt': 10, 'nov': 11, 'dez': 12
    }
    
    patterns = [
        (r'(\d{1,2})\.(\d{1,2})\.(\d{2,4})', 'numeric'),
        (r'(\d{1,2})\.\s*(Januar|Februar|Maerz|April|Mai|Juni|Juli|August|September|Oktober|November|Dezember)\s*(\d{4})', 'full_month'),
        (r'(\d{1,2})\.?\s+(Jan\.?|Feb\.?|MAr\.?|Maerz\.?|Mrz\.?|Apr\.?|Mai\.?|Jun\.?|Jul\.?|Aug\.?|Sep\.?|Sept\.?|Okt\.?|Nov\.?|Dez\.?)\s+(\d{4})', 'abbr_month'),
    ]
    
    # Pattern 1: DD.MM.YYYY
    for match in re.finditer(patterns[0][0], text):
        try:
            day = int(match.group(1))
            month = int(match.group(2))
            year_str = match.group(3)
            year = int(year_str) if len(year_str) == 4 else (2000 + int(year_str))
            
            date_obj = datetime(year, month, day)
            start = max(0, match.start() - 100)
            end = min(len(text), match.end() + 100)
            context = text[start:end].strip()
            
            dates_found.append((date_obj, context, match.group(0)))
        except ValueError:
            continue
    
    # Pattern 2: DD. Monat YYYY
    for match in re.finditer(patterns[1][0], text, re.IGNORECASE):
        try:
            day = int(match.group(1))
            month_name = match.group(2).lower()
            month = month_map_full.get(month_name)
            year = int(match.group(3))
            
            if month:
                date_obj = datetime(year, month, day)
                start = max(0, match.start() - 100)
                end = min(len(text), match.end() + 100)
                context = text[start:end].strip()
                
                dates_found.append((date_obj, context, match.group(0)))
        except ValueError:
            continue
    
    # Pattern 3: DD Mon. YYYY (abbreviated)
    for match in re.finditer(patterns[2][0], text, re.IGNORECASE):
        try:
            day = int(match.group(1))
            month_abbr = match.group(2).lower().replace('.', '').strip()
            month = month_map_abbr.get(month_abbr)
            year = int(match.group(3))
            
            if month:
                date_obj = datetime(year, month, day)
                start = max(0, match.start() - 100)
                end = min(len(text), match.end() + 100)
                context = text[start:end].strip()
                
                dates_found.append((date_obj, context, match.group(0)))
        except ValueError:
            continue
    
    return dates_found

def fetch_live_impuls_workshops() -> Optional[Dict]:
    """
    Fetch the live Impuls-Workshops page and return as a chunk.
    This ensures we always have the most current workshop information.
    """
    try:
        logger.info("FETCHING LIVE: Impuls-Workshops page from dlh.zh.ch")
        
        url = "https://dlh.zh.ch/home/impuls-workshops"
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        # Extract text content from HTML
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Get the main content text
        content = soup.get_text(separator=' ', strip=True)
        
        # Create a chunk in the same format as processed_chunks.json
        live_chunk = {
            'content': content,
            'metadata': {
                'source': url,
                'title': 'Impuls-Workshops - Digital Learning Hub Sek II (LIVE)',
                'fetched_live': True,
                'fetch_time': datetime.now().isoformat()
            }
        }
        
        logger.info(f"LIVE FETCH SUCCESS: Retrieved {len(content)} characters")
        return live_chunk
        
    except Exception as e:
        logger.error(f"LIVE FETCH FAILED: {str(e)}")
        return None

def fetch_live_innovationsfonds(subject: Optional[str] = None) -> Optional[Dict]:
    """
    Fetch live Innovationsfonds project pages.
    
    Args:
        subject: Subject name (e.g., "Chemie", "Mathematik") or None for overview
    
    Returns:
        Chunk with live data or None if fetch fails
    """
    # Mapping: Subject name -> URL slug
    subject_url_map = {
        'ABU': 'abu',
        'Architektur EFZ': 'architektur-efz',
        'Automobilberufe': 'automobilberufe',
        'Berufskunde': 'berufskunde',
        'Bildnerisches Gestalten': 'bildnerisches-gestalten',
        'Biologie': 'biologie',
        'Bruckenangebot': 'brueckenangebot',
        'Chemie': 'chemie',
        'Coiffeuse-Coiffeur': 'coiffeuse-coiffeur',
        'Deutsch': 'deutsch',
        'EBA': 'eba',
        'Elektroberufe': 'elektroberufe',
        'Englisch': 'englisch',
        'FaGe': 'fage',
        'Franzosisch': 'franzoesisch',
        'Geographie': 'geographie',
        'Geomatiker:innen EFZ': 'geomatiker-innen-efz',
        'Geschichte': 'geschichte',
        'Geschichte und Politik': 'geschichte-und-politik',
        'Griechisch': 'griechisch',
        'IKA': 'ika',
        'Informatik': 'informatik',
        'Italienisch': 'italienisch',
        'Landwirtschaftsmechaniker:innen': 'landwirtschaftsmechaniker-innen',
        'Latein': 'latein',
        'Mathematik': 'mathematik',
        'Maurer:innen': 'maurer-innen',
        'Physik': 'physik',
        'Russisch': 'russisch',
        'Schreiner:in': 'schreiner-in',
        'Sozialwissenschaften': 'sozialwissenschaften',
        'Spanisch': 'spanisch',
        'Sport': 'sport',
        'Uberfachlich': 'ueberfachlich',
        'Wirtschaft': 'wirtschaft'
    }
    
    try:
        # Build URL based on subject
        base_url = "https://dlh.zh.ch/home/innovationsfonds/projektvorstellungen/uebersicht"
        
        if subject and subject in subject_url_map:
            url_slug = subject_url_map[subject]
            url = f"{base_url}/filterergebnisse-fuer-projekte/tags/{url_slug}"
            logger.info(f"FETCHING LIVE: Innovationsfonds {subject} projects from dlh.zh.ch")
        else:
            url = base_url
            logger.info(f"FETCHING LIVE: Innovationsfonds overview from dlh.zh.ch")
        
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        # Extract text content from HTML
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')
        content = soup.get_text(separator=' ', strip=True)
        
        # Create chunk
        title_suffix = f" - {subject}" if subject else " - Ubersicht"
        live_chunk = {
            'content': content,
            'metadata': {
                'source': url,
                'title': f'Innovationsfonds Projekte{title_suffix} (LIVE)',
                'fetched_live': True,
                'fetch_time': datetime.now().isoformat(),
                'faecher': [subject] if subject else []
            }
        }
        
        logger.info(f"LIVE FETCH SUCCESS: Retrieved {len(content)} characters")
        return live_chunk
        
    except Exception as e:
        logger.error(f"LIVE FETCH FAILED: {str(e)}")
        return None

def sort_events_chronologically(chunks: List[Dict], current_date: datetime = None) -> Dict[str, List[Dict]]:
    """Sortiere Events chronologisch"""
    if current_date is None:
        current_date = datetime.now()
    
    future_events = []
    past_events = []
    no_date_events = []
    
    for chunk in chunks:
        content = chunk['content']
        dates = extract_dates_from_text(content)
        
        if dates:
            dates.sort(key=lambda x: x[0])
            earliest_date = dates[0][0]
            
            event_info = {
                'chunk': chunk,
                'date': earliest_date,
                'date_str': dates[0][2],
                'all_dates': dates,
                'context': dates[0][1]
            }
            
            if earliest_date.date() < current_date.date():
                past_events.append(event_info)
            else:
                future_events.append(event_info)
        else:
            no_date_events.append({'chunk': chunk})
    
    future_events.sort(key=lambda x: x['date'])
    past_events.sort(key=lambda x: x['date'], reverse=True)
    
    return {
        'future_events': future_events,
        'past_events': past_events,
        'no_date_events': no_date_events
    }

def extract_query_intent(query: str) -> Dict[str, any]:
    """Analysiere die Absicht der Frage"""
    query_lower = query.lower()
    
    innovationsfonds_terms = [
        'innovationsfonds', 'innovationsprojekt', 'innovationsprojekte',
        'projekt', 'projekte', 'welche projekte'
    ]
    
    intent = {
        'is_date_query': any(term in query_lower for term in ['heute', 'morgen', 'termin', 'wann', 'datum', 'zeit', 'event', 'veranstaltung', 'naechste', 'kommende']),
        'is_how_to': any(term in query_lower for term in ['wie', 'anleitung', 'tutorial', 'schritte']),
        'is_definition': any(term in query_lower for term in ['was ist', 'was sind', 'definition', 'bedeutung']),
        'wants_list': any(term in query_lower for term in ['welche', 'liste', 'alle', 'ueberblick', 'uebersicht']),
        'wants_contact': any(term in query_lower for term in ['kontakt', 'anmeldung', 'email', 'telefon', 'anmelden']),
        'is_innovationsfonds_query': any(term in query_lower for term in innovationsfonds_terms),
        'topic_keywords': [],
        'subject_keywords': []
    }
    
    topics = {
        'fobizz': ['fobizz', 'to teach', 'to-teach'],
        'genki': ['genki', 'gen ki', 'gen-ki'],
        'innovationsfonds': ['innovationsfonds', 'innovation', 'projekt'],
        'workshop': ['workshop', 'impuls', 'veranstaltung', 'impuls-workshop', 'impulsworkshop', 'event'],
        'cop': ['cop', 'cops', 'community', 'practice'],
        'weiterbildung': ['weiterbildung', 'fortbildung', 'kurs', 'schulung'],
        'vernetzung': ['vernetzung', 'netzwerk', 'austausch'],
        'kuratiert': ['kuratiert', 'kuratiertes', 'sammlung']
    }
    
    subjects = {
        'chemie': ['chemie'],
        'physik': ['physik'],
        'biologie': ['biologie'],
        'mathematik': ['mathematik', 'mathe'],
        'informatik': ['informatik'],
        'deutsch': ['deutsch'],
        'englisch': ['englisch'],
        'franzAsisch': ['franzAsisch'],
        'italienisch': ['italienisch'],
        'spanisch': ['spanisch'],
        'geschichte': ['geschichte'],
        'geografie': ['geografie', 'geographie'],
        'wirtschaft': ['wirtschaft'],
        'recht': ['recht'],
        'philosophie': ['philosophie']
    }
    
    for topic, keywords in topics.items():
        if any(kw in query_lower for kw in keywords):
            intent['topic_keywords'].append(topic)
    
    for subject, keywords in subjects.items():
        if any(kw in query_lower for kw in keywords):
            intent['subject_keywords'].append(subject)
    
    return intent

def advanced_search(query: str, max_results: int = 10) -> List[Dict]:
    """BEST OF BOTH: Metadata search + Overview URL prioritization"""
    intent = extract_query_intent(query)
    query_lower = query.lower()
    query_words = set(query_lower.split())
    
    results = []
    
    if 'impuls' in query_lower and 'workshop' in query_lower:
        intent['topic_keywords'].append('impulsworkshop')
    
    # FROM OLD VERSION: Overview URL Prioritization!
    overview_urls = [
        'https://dlh.zh.ch/',
        'https://dlh.zh.ch',
        'https://dlh.zh.ch/home/impuls-workshops',
        'https://dlh.zh.ch/home/aktuelle-termine',
        'https://dlh.zh.ch/home/innovationsfonds/projektvorstellungen/uebersicht'
    ]
    
    # LIVE FETCH: For workshop/event queries, ALWAYS fetch the live page
    if intent['is_date_query'] or any(k in query_lower for k in ['impuls', 'workshop', 'termine', 'veranstaltung', 'events']):
        print(f"LIVE FETCH: Fetching current Impuls-Workshops page")
        live_chunk = fetch_live_impuls_workshops()
        if live_chunk:
            # Add live data with HIGHEST priority (200)
            results.append((200, live_chunk))
            print(f"   LIVE DATA: Added with score 200 (highest priority)")
    
    # PRIORITAT 1: Bei Event/Workshop-Anfragen die Abersichtsseiten ZUERST! (Score 150)
    if intent['is_date_query'] or any(k in query_lower for k in ['impuls', 'workshop', 'termine', 'veranstaltung', 'events']):
        print(f"Y Prioritizing overview pages for workshop/event query")
        for overview_url in overview_urls:
            if overview_url in URL_INDEX:
                for idx in URL_INDEX[overview_url][:2]:
                    if idx < len(CHUNKS):
                        results.append((150, CHUNKS[idx]))
                        print(f"   Added overview page with score 150")
    
    # LIVE FETCH: For Innovationsfonds queries, ALWAYS fetch live data
    if intent['is_innovationsfonds_query']:
        if intent['subject_keywords']:
            # Subject-specific query: fetch each subject's page
            print(f"LIVE FETCH: Innovationsfonds projects for subjects: {intent['subject_keywords']}")
            for subject in intent['subject_keywords']:
                # Map lowercase subject back to proper case
                subject_map = {
                    'abu': 'ABU',
                    'architektur': 'Architektur EFZ',
                    'automobilberufe': 'Automobilberufe',
                    'berufskunde': 'Berufskunde',
                    'bildnerisches gestalten': 'Bildnerisches Gestalten',
                    'biologie': 'Biologie',
                    'brueckenangebot': 'Bruckenangebot',
                    'chemie': 'Chemie',
                    'coiffeuse': 'Coiffeuse-Coiffeur',
                    'deutsch': 'Deutsch',
                    'eba': 'EBA',
                    'elektroberufe': 'Elektroberufe',
                    'englisch': 'Englisch',
                    'fage': 'FaGe',
                    'franzoesisch': 'Franzosisch',
                    'geographie': 'Geographie',
                    'geomatiker': 'Geomatiker:innen EFZ',
                    'geschichte': 'Geschichte',
                    'geschichte und politik': 'Geschichte und Politik',
                    'griechisch': 'Griechisch',
                    'ika': 'IKA',
                    'informatik': 'Informatik',
                    'italienisch': 'Italienisch',
                    'landwirtschaftsmechaniker': 'Landwirtschaftsmechaniker:innen',
                    'latein': 'Latein',
                    'mathematik': 'Mathematik',
                    'maurer': 'Maurer:innen',
                    'physik': 'Physik',
                    'russisch': 'Russisch',
                    'schreiner': 'Schreiner:in',
                    'sozialwissenschaften': 'Sozialwissenschaften',
                    'spanisch': 'Spanisch',
                    'sport': 'Sport',
                    'ueberfachlich': 'Uberfachlich',
                    'wirtschaft': 'Wirtschaft'
                }
                proper_subject = subject_map.get(subject, subject.capitalize())
                live_chunk = fetch_live_innovationsfonds(subject=proper_subject)
                if live_chunk:
                    results.append((210, live_chunk))  # Priority 210 - highest!
                    print(f"   LIVE DATA: Added {proper_subject} projects with score 210")
        else:
            # General query: fetch overview page
            print(f"LIVE FETCH: Innovationsfonds overview page")
            live_chunk = fetch_live_innovationsfonds(subject=None)
            if live_chunk:
                results.append((210, live_chunk))  # Priority 210 - highest!
                print(f"   LIVE DATA: Added overview with score 210")
    
    # PRIORITAT 2: Metadaten-basierte Fachsuche fuer Innovationsfonds (Score 200)
    if intent['is_innovationsfonds_query'] and intent['subject_keywords']:
        print(f"Y Searching for Innovationsfonds projects in subjects: {intent['subject_keywords']}")
        for subject in intent['subject_keywords']:
            if subject in SUBJECT_INDEX:
                print(f"   Found {len(SUBJECT_INDEX[subject])} projects in {subject} via metadata")
                for idx in SUBJECT_INDEX[subject]:
                    if idx < len(CHUNKS):
                        chunk = CHUNKS[idx]
                        if 'projektvorstellungen' in chunk['metadata'].get('source', '').lower():
                            if not any(r[1] == chunk for r in results):
                                results.append((200, chunk))
    
    # PRIORITAT 3: Allgemeine Innovationsfonds-Anfragen (Score 150)
    elif intent['is_innovationsfonds_query']:
        for url, indices in URL_INDEX.items():
            if 'projektvorstellungen' in url:
                for idx in indices:
                    if idx < len(CHUNKS):
                        chunk = CHUNKS[idx]
                        if not any(r[1] == chunk for r in results):
                            results.append((150, chunk))
    
    # 4. URL-Treffer (vermeidet overview_urls Duplikate!)
    for topic in intent['topic_keywords']:
        for url, indices in URL_INDEX.items():
            if topic in url and url not in overview_urls:
                for idx in indices[:3]:
                    if idx < len(CHUNKS):
                        chunk = CHUNKS[idx]
                        if not any(r[1] == chunk for r in results):
                            results.append((100, chunk))
    
    # 5. Keyword-Index-Suche
    for topic in intent['topic_keywords'] + intent['subject_keywords']:
        if topic in KEYWORD_INDEX:
            for idx in KEYWORD_INDEX[topic][:5]:
                if idx < len(CHUNKS):
                    chunk = CHUNKS[idx]
                    if not any(r[1] == chunk for r in results):
                        results.append((80, chunk))
    
    # 6. Textsuche
    for i, chunk in enumerate(CHUNKS):
        if len(results) > max_results * 3:
            break
            
        content_lower = chunk['content'].lower()
        score = 0
        
        if 'projektvorstellungen' in chunk['metadata'].get('source', '').lower():
            score += 30
        
        if len(query_words) > 1:
            words_list = query_lower.split()
            for j in range(len(words_list) - 1):
                phrase = f"{words_list[j]} {words_list[j+1]}"
                if phrase in content_lower:
                    score += 25
        
        content_words = set(content_lower.split())
        matching_words = query_words & content_words
        score += len(matching_words) * 5
        
        if intent['is_date_query'] and any(d in content_lower for d in ['2024', '2025', '2026', 'uhr', 'datum', 'termin']):
            score += 20
        
        if score > 10 and not any(r[1] == chunk for r in results):
            results.append((score, chunk))
    
    results.sort(key=lambda x: x[0], reverse=True)
    
    print(f"YS Search results before deduplication: {len(results)}")
    if results:
        print(f"   Top 5 scores: {[r[0] for r in results[:5]]}")
    
    # URL-basierte Deduplizierung
    final_results = []
    seen_urls = set()
    url_count = Counter()
    
    for score, chunk in results:
        url = chunk['metadata'].get('source', '')
        is_project_page = 'projektvorstellungen' in url.lower() and 'uebersicht' in url.lower() and any(char.isdigit() for char in url.split('/')[-1])
        
        if is_project_page:
            if url not in seen_urls:
                final_results.append(chunk)
                seen_urls.add(url)
        else:
            if url_count[url] < 2:
                final_results.append(chunk)
                url_count[url] += 1
            
        if len(final_results) >= max_results:
            break
    
    print(f" Final results: {len(final_results)}")
    
    return final_results

def create_enhanced_prompt(question: str, chunks: List[Dict], intent: Dict) -> str:
    """Erstelle Prompt - Formatierung ist im System Prompt"""
    
    current_date = datetime.now()
    current_date_str = current_date.strftime('%d.%m.%Y')
    
    # Event-Sortierung von gestern!
    if intent['is_date_query'] or any(keyword in ['workshop', 'veranstaltung'] for keyword in intent['topic_keywords']):
        sorted_events = sort_events_chronologically(chunks, current_date)
        
        context_parts = []
        
        if sorted_events['future_events']:
            context_parts.append("=== KOMMENDE VERANSTALTUNGEN (chronologisch sortiert) ===")
            for event in sorted_events['future_events']:
                days_until = (event['date'].date() - current_date.date()).days
                context_parts.append(f"\nY... DATUM: {event['date'].strftime('%d.%m.%Y (%A)')} (in {days_until} Tagen)")
                context_parts.append(f"Titel: {event['chunk']['metadata'].get('title', 'Unbekannt')}")
                context_parts.append(f"Quelle: {event['chunk']['metadata'].get('source', 'Unbekannt')}")
                context_parts.append(event['chunk']['content'][:400])
                context_parts.append("---")
        
        if sorted_events['past_events']:
            context_parts.append("\n\n=== VERGANGENE VERANSTALTUNGEN ===")
            for event in sorted_events['past_events'][:5]:
                days_ago = (current_date.date() - event['date'].date()).days
                context_parts.append(f"\nY... DATUM: {event['date'].strftime('%d.%m.%Y (%A)')} (vor {days_ago} Tagen - BEREITS VORBEI)")
                context_parts.append(f"Titel: {event['chunk']['metadata'].get('title', 'Unbekannt')}")
                context_parts.append(f"Quelle: {event['chunk']['metadata'].get('source', 'Unbekannt')}")
                context_parts.append(event['chunk']['content'][:400])
                context_parts.append("---")
        
        if sorted_events['no_date_events']:
            context_parts.append("\n\n=== WEITERE INFORMATIONEN ===")
            for item in sorted_events['no_date_events']:
                context_parts.append(f"\nTitel: {item['chunk']['metadata'].get('title', 'Unbekannt')}")
                context_parts.append(f"Quelle: {item['chunk']['metadata'].get('source', 'Unbekannt')}")
                context_parts.append(item['chunk']['content'][:400])
                context_parts.append("---")
        
        context = "\n".join(context_parts)
        
        prompt = f"""Heutiges Datum: {current_date_str}

KONTEXT (chronologisch sortierte Veranstaltungen):
{context}

FRAGE: {question}

Beantworte die Frage. Zeige zukA14nftige Events zuerst, chronologisch sortiert."""

    else:
        # Projekte
        context_parts = []
        for chunk in chunks:
            url = chunk['metadata'].get('source', 'Unbekannt')
            title = chunk['metadata'].get('title', 'Keine Beschreibung')
            faecher = chunk['metadata'].get('faecher', [])
            
            context_parts.append(f"=== Projekt: {title} ===")
            context_parts.append(f"URL: {url}")
            if faecher:
                context_parts.append(f"FAcher: {', '.join(faecher)}")
            context_parts.append(chunk['content'][:400])
            context_parts.append("---\n")
        
        context = "\n".join(context_parts)
        
        if intent['is_innovationsfonds_query']:
            prompt = f"""KONTEXT (Innovationsfonds-Projekte mit URLs):
{context}

FRAGE: {question}

Zeige JEDEN Projekttitel als klickbaren Link (Format siehe System-Prompt)."""
        else:
            prompt = f"""KONTEXT:
{context}

FRAGE: {question}"""
    
    return prompt

@app.get("/")
async def root():
    return {
        "message": "DLH Chatbot API (Best of Both Worlds)",
        "status": "running" if len(CHUNKS) > 0 else "ERROR",
        "chunks_loaded": len(CHUNKS),
        "version": "4.0.0"
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy" if len(CHUNKS) > 0 else "ERROR",
        "chunks_loaded": len(CHUNKS),
        "features": "System Prompt + Metadata Search + Overview Priority + Event Sorting"
    }

@app.post("/ask", response_model=AnswerResponse)
async def ask_question(request: QuestionRequest):
    """Beantworte Fragen"""
    
    if len(CHUNKS) == 0:
        raise HTTPException(status_code=500, detail="No data loaded")
    
    try:
        intent = extract_query_intent(request.question)
        
        print(f"\nY Query: {request.question}")
        print(f"   Intent: date={intent['is_date_query']}, innovationsfonds={intent['is_innovationsfonds_query']}")
        print(f"   Topics: {intent['topic_keywords']}, Subjects: {intent['subject_keywords']}")
        
        relevant_chunks = advanced_search(request.question, max_results=request.max_sources + 5)
        
        if not relevant_chunks:
            answer = f"<strong>Entschuldigung, ich konnte keine relevanten Informationen finden.</strong><br><br>"
            answer += "Besuchen Sie <a href='https://dlh.zh.ch' target='_blank'>dlh.zh.ch</a> fuer weitere Informationen."
            
            return AnswerResponse(question=request.question, answer=answer, sources=[])
        
        prompt = create_enhanced_prompt(request.question, relevant_chunks, intent)
        
        # System Prompt fuer garantierte Formatierung!
        system_prompt = """Du bist der offizielle DLH Chatbot. Antworte auf Deutsch mit HTML-Formatierung. Liste bei Terminfragen die nächsten 5–10 Einträge als einzelne Zeilen mit Datum, Zeit, Titel (verlinkt).

KRITISCHE REGEL - PROJEKTTITEL MASSEN IMMER KLICKBARE LINKS SEIN:
Format: <strong><a href="VOLLSTANDIGE-URL" target="_blank">Projekttitel</a></strong><br>
Beschreibung in 1-2 SAtzen<br><br>

HTML-Tags:
- <br> = Zeilenumbruch
- <br><br> = Absatz
- <strong> = Aberschriften
- <a href="URL" target="_blank"> = Links
- NIEMALS Markdown (*, #, _)

BEISPIEL:
<strong><a href="https://dlh.zh.ch/home/innovationsfonds/projektvorstellungen/uebersicht/1042-histoswiss" target="_blank">HistoSwiss</a></strong><br>
InterdisziplinAres Geschichtsprojekt mit digitalen Werkzeugen<br><br>"""

        try:
            response = anthropic_client.messages.create(
                model="claude-sonnet-4-5-20250929",  # Sonnet 4.5!
                max_tokens=2500,
                temperature=0.3,
                system=system_prompt,
                messages=[{"role": "user", "content": prompt}]
            )
            
            answer = response.content[0].text
            
        except Exception as claude_error:
            print(f"Y  Claude API Error: {claude_error}")
            
            answer = "<strong>Entschuldigung, ich kann gerade nicht auf die KI zugreifen.</strong><br><br>"
            for i, chunk in enumerate(relevant_chunks[:3]):
                title = chunk['metadata'].get('title', 'Information')
                url = chunk['metadata'].get('source', '')
                answer += f"<strong><a href='{url}' target='_blank'>{title}</a></strong><br><br>"
        
        # Format sources
        sources = []
        seen_urls = set()
        for chunk in relevant_chunks[:request.max_sources]:
            url = chunk['metadata']['source']
            if url not in seen_urls:
                sources.append(Source(
                    url=url,
                    title=chunk['metadata'].get('title', 'DLH Seite'),
                    snippet=chunk['content'][:150] + "..."
                ))
                seen_urls.add(url)
        
        return AnswerResponse(question=request.question, answer=answer, sources=sources)
        
    except Exception as e:
        print(f"Error: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    print("\n Starting DLH Chatbot API (BEST OF BOTH WORLDS)...")
    print(f"Ys Loaded {len(CHUNKS)} chunks")
    print(f"Y Indexed {len(KEYWORD_INDEX)} keywords")
    print(f"Ys Indexed {len(SUBJECT_INDEX)} subjects")
    print(" Features:")
    print("    System Prompt (guaranteed links)")
    print("    Metadata subject search")
    print("    Overview URL prioritization")
    print("    Event chronological sorting")
    print("    Sonnet 4.5")
    print("\n ALL FEATURES ENABLED!\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)
